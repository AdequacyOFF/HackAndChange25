## Отчёт по работе: построение русскоязычной модели анализа тональности на основе ModernBERT

### 1. Постановка задачи и мотивация

Цель работы — получить современную, но при этом достаточно лёгкую и быструю модель для анализа тональности русскоязычных текстов.

С одной стороны, сейчас уже есть много сильных моделей для русского: крупные мультиязычные трансформеры, специализированные энкодеры и т.д. Но у них есть ряд практических минусов:

* высокая ресурсоёмкость (нужен серьёзный GPU, много памяти);
* сложность развёртывания в продакшене;
* избыточность по размеру для задач «просто классифицировать тональность текста».

С другой стороны, «лёгкие» модели часто построены на устаревших архитектурах или обучены на небольших, узких по домену датасетах — и в результате проигрывают по качеству.

Мы берём за основу современную архитектуру ModernBERT (mmBERT — мультиязычный вариант) и строим вокруг неё аккуратный pipeline работы с данными:

* собираем крупный русскоязычный корпус,
* выжимаем из него максимум информации с помощью кластеризации и псевдоразметки,
* и на последнем шаге обучаем модель одновременно как классификатор и как metric-learning систему.

Основной упор — на процесс: как именно мы двигаемся от «сырого» массива датасетов к финальной модели.

---

### 2. Базовая модель: ModernBERT / mmBERT

В качестве основы использован мультиязычный ModernBERT (mmBERT):

* ~307M параметров;
* контекст до 8192 токенов;
* словарь около 256k токенов (на базе Gemma2);
* встроенная поддержка русского языка среди большого количества других.

Почему именно он:

1. **Современная архитектура.** Это уже «обновлённый» BERT с учётом последних практик (оптимизации, архитектурные улучшения).
2. **Длинный контекст.** Реальные тексты часто длинные, особенно отзывы, новости, посты.
3. **Мультиязычность с русским.** Уже есть базовое понимание русского языка, нет необходимости полностью претренировать модель с нуля.

Важно: мы не меняем архитектуру, а строим процесс вокруг неё — всё качество «добывается» через данные и схему обучения.

---

### 3. Данные: что мы собрали

Мы собрали единый корпус из множества русскоязычных (и околорусскоязычных) датасетов. Среди них, в частности:

* **Классические sentiment-датасеты:**

  * Ru-Sentiment;
  * clapAI / MultiLingualSentiment;
  * Ru-Reviews.

* **Отзывы, комментарии, твиты, новости:**

  * GeoReview (классификация и кластеризация);
  * Kinopoisk;
  * Ru-TweetCorp;
  * крупный 1m Corpus от Яндекса;
  * различные новостные и тематические корпуса (RIA и др.).

* **Специализированные и вспомогательные датасеты:**

  * CEDR;
  * несколько корпусов Ru-SciBench;
  * Ru-STS;
  * Sensitive Topics Classification;
  * Terra Pair Classification и др.

Совокупный объём — более **2,4 млн текстов**.

На этом этапе важно понимать:

* датасеты разнородны по задачам (тональность, тематическая классификация, стили, семантическая близость и т.п.);
* разметка очень неоднородна по качеству;
* множество наборов не являются чистыми датасетами по тональности.

Прямо «скормить всё» модели — это гарантированный путь получить много шума и непонятно какие сдвиги в распределении классов. Поэтому центральная идея — **использовать сильный кусок данных как «якорь», а всё остальное — аккуратно доразмечать и фильтровать.**

---

### 4. Общая схема процесса

Весь pipeline можно разделить на три крупных шага:

1. **Обучение базового классификатора тональности**
   На небольшом, но максимально качественном подмножестве данных по тональности мы обучаем первую версию модели. Она выступает в роли «эксперта» для дальнейших шагов.

2. **Кластеризация и псевдоразметка большого корпуса**
   Используем базовый классификатор, чтобы:

   * получить эмбеддинги всех текстов;
   * рассчитать для них вероятности классов;
   * сгруппировать их в кластеры;
   * измерить «чистоту» кластеров и отфильтровать наиболее надёжные зоны данных.

3. **Обучение финальной модели с metric learning**
   На основе:

   * изначальных качественно размеченных данных,
   * плюс отфильтрованных псевдоразмеченных данных
     обучаем модель с комбинированной функцией потерь: классификация + метрическая (контрастивная) часть.

Ниже — детали каждого шага.

---

### 5. Шаг 1. Обучение базового классификатора

#### 5.1. Выбор данных

Для первой модели нам нужна максимально «надёжная» разметка именно по тональности. Поэтому мы берём только датасеты, где:

* метки тональности хорошо определены;
* разметка либо ручная, либо близкая к этому по качеству.

В итоге используем:

* **Ru-Sentiment** — классический русскоязычный датасет по тональности;
* **clapAI / MultiLingualSentiment** — мультиязычные отзывы с метками;
* **Ru-Reviews** — отзывы, хорошо подходящие под задачу sentiment analysis.

Все они приводятся к единому формату: трёхклассовая тональность (`negative / neutral / positive`).

#### 5.2. Обучение

* В качестве модели: mmBERT + классификационная голова.
* Целевая задача: предсказать класс тональности.
* Оптимизатор, scheduler и т.п.; центральная часть — именно в данных, а не в хитрой архитектуре.

В процессе обучения:

* следим за F1-метрикой (macro-F1) по валидации;
* строим confusion-матрицы, чтобы видеть, где именно модель путается (обычно это границы между нейтральным и слабым позитивом/негативом).

По кривой F1 видно:

* качество растёт, достигает пика (примерно F1 ≈ 0.76),
* затем начинает плавно деградировать — признак переобучения.

Мы фиксируем **лучший чекпоинт по валидации** и используем именно его как базовый классификатор для следующего шага.

---

### 6. Шаг 2. Кластеризация и псевдоразметка

Задача этого шага — «упорядочить» весь большой корпус, используя базовую модель как инструмент.

#### 6.1. Получение эмбеддингов

Для каждого текста из большого корпуса мы:

1. Прогоняем его через базовый классификатор.
2. Сохраняем:

   * CLS-вектор;
   * вектор вероятностей по классам (`p_neg`, `p_neu`, `p_pos`);
   * предсказанную метку класса (по argmax).

Получаем гигантский банк эмбеддингов + предсказаний.

Чтобы ускорить дальнейшие шаги:

* нормализуем эмбеддинги по L2;
* обучаем PCA и снижаем размерность до, например, **128**.
  Это:

  * уменьшает потребление памяти;
  * делает кластеризацию быстрее и стабильнее.

#### 6.2. Обучение K-Means

На PCA-векторах обучаем **K-Means**:

* количество кластеров подбирается экспериментально;
* рабочий вариант — **2000 кластеров**: баланс между детализацией и устойчивостью.

После обучения у нас есть:

* центроид каждого кластера;
* принадлежность каждого текста к кластеру.

#### 6.3. Оценка чистоты кластеров (purity)

Кластер сам по себе ещё ничего не говорит о качестве. Нам важно понять, насколько он однороден по тональности.

Для каждого кластера:

1. Берём все тексты, попавшие в него.
2. Смотрим на **предсказанные моделью** метки классов.
3. Считаем **purity**: доля объектов доминирующего класса в этом кластере.

Пример:

* в кластере 100 текстов;
* модель предсказала:

  * 80 — `positive`,
  * 15 — `neutral`,
  * 5 — `negative`.
* purity = 80 / 100 = 0.8.

Интуитивно:

* **высокая purity** ⇒ кластер относительно «чистый», модель внутри него уверена и последовательна;
* **низкая purity** ⇒ кластер смешанный, его псевдоразметка ненадёжна.

#### 6.4. Фильтрация данных

Теперь у нас на каждый текст есть:

* кластер;
* метка модели;
* вектор вероятностей;
* значение purity для его кластера.

Мы вводим два типа порогов:

1. **По кластеру:** минимальная допустимая purity (0.9).
2. **По уверенности модели:** минимальная вероятность по предсказанному классу (0.8).

Тексты, которые:

* лежат в кластерах с высокой чистотой,
* и у которых модель уверена (высокая вероятность по предсказанному классу),

мы считаем **надёжно псевдоразмеченными**.

Такие объекты можно использовать в дальнейших шагах практически как «полноценную» разметку, но с пониманием, что она чуть менее надёжна, чем исходная ручная.

Результат шага 2:

* большой корпус превращён в структурированный набор:

  * чистые кластеры с надёжной псевдоразметкой;
  * грязные или неоднородные кластеры, которые мы либо отбрасываем, либо используем с осторожностью.

---

### 7. Шаг 3. Обучение финальной модели с metric learning

Теперь у нас есть:

* исходная «золотая» разметка по тональности (шаг 1),
* отфильтрованные псевдоразмеченные данные из шаг 2.

На этом мы строим финальную модель.

#### 7.1. Зачем вообще metric learning?

Обычная классификация учит модель:

> правильно ставить метку классу.

Но она **не гарантирует**, что внутренние представления классов будут хорошо структурированы. Можно получить ситуацию, когда:

* модель по argmax даёт правильную метку,
* но векторные представления разных классов сильно наслаиваются друг на друга.

Metric learning добавляет дополнительное требование:

* объекты одного класса должны быть **близки** в эмбеддинг-пространстве;
* объекты разных классов — **далеки**.

Это делает:

* пространство более «чистым»;
* модель более устойчивой к шуму;
* и улучшает обобщающую способность, особенно на новых доменах.

#### 7.2. Формирование обучающей выборки для metric learning

Используем комбинацию:

* «золотых» размеченных примеров (Ru-Sentiment, Ru-Reviews, и т.д.);
* плюс псевдоразмеченных из чистых кластеров.

На основе этих данных строим:

* **положительные пары/группы**: тексты одного класса, желательно из одного кластера;
* **отрицательные пары**: тексты разных классов (часто — из кластеров с разной доминирующей тональностью).

Таким образом:

* кластеры дают «естественные» группы похожих текстов,
* а исходная разметка помогает «заземлить» всё по реальным меткам.

#### 7.3. Функция потерь

Финальная модель обучается по комбинированной функции потерь:

* **классификационная часть** — кросс-энтропия по трём классам тональности;
* **метрическая часть** — supervised contrastive loss или аналогичная loss-функция, которая:

  * сближает эмбеддинги внутри класса;
  * отдаляет эмбеддинги между классами.

Суммарный loss =
`L_total = λ_class * L_ce + λ_metric * L_supcon`

где веса λ позволяют балансировать вклад двух частей.

На практике:

* если дать слишком большой вес metric-части, модель может начать «портить» границы классов;
* если вес слишком мал, польза от metric learning почти исчезнет.

Поэтому веса подбираются эмпирически по валидации.

#### 7.4. Обучение и мониторинг

В ходе обучения финальной модели мы:

* отслеживаем F1, accuracy и другие метрики на:

  * «чистой» валидации (из исходной ручной разметки),
  * и при необходимости — на дополнительной отложенной выборке;
* смотрим на confusion-матрицы:

  * насколько уменьшилось количество «пограничных» ошибок;
  * не появилось ли смещение (например, сильная переориентация в сторону какого-то класса).

По итогам обучения выбираем лучший чекпоинт по F1 (**0.9**) и фиксируем его как финальную модель.

---

### 8. Результаты и наблюдения

Ключевые наблюдения:

1. **Качество растёт не за счёт архитектуры, а за счёт процесса.**
   Мы изначально берём «готовый» ModernBERT и не усложняем модель, а работаем с данными и обучением.

2. **Кластеризация + псевдоразметка позволяют масштабироваться.**
   Вместо того, чтобы ограничиваться только небольшими вручную размеченными наборами, мы аккуратно «подтягиваем» огромный массив сырых данных, но фильтруем его по чистоте кластеров и уверенности модели.

3. **Metric learning улучшает структуру пространства.**
   Это отражается:

   * на confusion-матрицах (меньше путаницы между близкими классами),
   * на устойчивости к шуму и смещению домена.

4. **Финальная модель даёт высокое качество (F1 ~ 0.9)** при умеренном размере и возможности реального применения.

---

### 9. Ограничения и дальнейшая работа

У подхода есть ограничения:

* качество псевдоразметки всегда упирается в качество базовой модели;
* выбор порогов purity и уверенности — эмпирический компромисс;
* кластеризация (особенно на миллионах текстов) предъявляет серьёзные требования к инфраструктуре.

Планы развития:

1. **Расширение корпуса.**
   Добавление новых доменов: соцсети, специализированные форумы, узкопрофессиональные отзывы.

2. **Уточнение схемы metric learning.**

   * более сложные loss-функции;
   * умное формирование «hard» позитивных и негативных примеров.
